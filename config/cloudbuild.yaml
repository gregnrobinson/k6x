steps:
  - name: gcr.io/${_PROJECT_ID}/k6:latest
    id: Retrieve Latency Info and Clean Data
    entrypoint: bash
    args:
      - -c
      - |
        GCP_SA_JSON=$(find ./creds/*.json)
        gcloud auth activate-service-account --key-file=$${GCP_SA_JSON}
        gcloud config set project $PROJECT_ID

        echo "******************************************"
        echo "Generate K6 Recipe"
        echo "******************************************"

        cat > settings <<HERE
          duration: '${_K6_DURATION}',
          vus: ${_K6_VUS},
        HERE

        sed -i "/options/r settings" recipe.js

        cat urls | while read line || [[ -n $line ]];
        do
        cat >> k6_urls <<HERE
          res = http.get("${line}");
          trackDataMetricsPerURL(res);
        HERE
        done

        sed -i "/let res;/r k6_urls" recipe.js

        echo "******************************************"
        echo "Collect Metrics"
        echo "******************************************"

        k6 run --insecure-skip-tls-verify --summary-time-unit=ms --out json=${_GCS_FILE_NAME} recipe.js

        #        #echo "******************************************"
        #        #echo "Clean K6 JSON Output"
        #        #echo "******************************************"
        #
        #        #sed -i '/vus/d; /gauge/d; /trend/d; /group_duration/d; /iterations/d; /iteration_duration/d; /counter/d; /rate/d; /"metric":"data_received"/d; /"metric":"data_sent"/d' ${_GCS_FILE_NAME}
        #        #cat ${_GCS_FILE_NAME} | jq -c '.data.tags += {"id":''"'${_TEST_NAME}'"}' | sponge ${_GCS_FILE_NAME}
        #
        #        echo "******************************************"
        #        echo "Load Dataset Into BigQuery"
        #        echo "******************************************"
        #        bq ls --filter labels.name:${_BQ_DATASET_NAME} --project_id $PROJECT_ID
        #        if [ -n "$(bq ls --filter labels.name:${_BQ_DATASET_NAME} --project_id $PROJECT_ID)" ]; then
        #          bq update ${PROJECT_ID}:${_BQ_DATASET_NAME}.${_BQ_TABLE_NAME} ./schema.json
        #          bq load \
        #            --location=${_LOCATION} \
        #            --source_format=${_BQ_DATASET_FORMAT} \
        #            --schema=./schema.json \
        #            --replace=false ${_BQ_DATASET_NAME}.${_BQ_TABLE_NAME} ${_GCS_FILE_NAME}
        #        else
        #          echo "BIGQUERY DATASET DOES NOT EXIST... CREATING..."
        #          bq mk \
        #            --location=${_LOCATION} \
        #            --dataset \
        #            --default_table_expiration 0 \
        #            --description "${_BQ_DATASET_DESC}" \
        #            ${PROJECT_ID}:${_BQ_DATASET_NAME}
        #          bq update --set_label name:${_BQ_DATASET_NAME} ${PROJECT_ID}:${_BQ_DATASET_NAME}
        #          echo "LOADING DATASET INTO BIGQUERY..."
        #          bq load \
        #            --location=${_LOCATION} \
        #            --source_format=${_BQ_DATASET_FORMAT} \
        #            --schema=./schema.json \
        #            --replace=false ${_BQ_DATASET_NAME}.${_BQ_TABLE_NAME} ${_GCS_FILE_NAME}
        #        fi
        #
        #echo "******************************************"
        #echo "Upload Dataset to GCS"
        #echo "******************************************"
        #
        #if [ -n "$(gsutil ls gs://${_GCS_BUCKET_NAME}/)" ]; then
        #  gsutil cp ./${_GCS_FILE_NAME} gs://${_GCS_BUCKET_NAME}/${_TEST_NAME}/`date +"%m-%d-%Y-%H%M%S"`.json
        #else
        #  echo "BUCKET DOES NOT EXIST... CREATING..."
        #  gsutil mb -l ${_LOCATION} gs://${_GCS_BUCKET_NAME}/
        #
        #  echo "UPLOADING DATASET TO GCS..."
        #  gsutil cp ./${_GCS_FILE_NAME} gs://${_GCS_BUCKET_NAME}/${_TEST_NAME}/`date +"%m-%d-%Y-%H%M%S"`.json
        #fi
        #
        #echo "******************************************"
        #echo "Done"
        #echo "******************************************"
timeout: 1200s
substitutions:
  _PROJECT_ID: "greg-apigee-hybrid-eks"
  _TEST_NAME: "null"
  _LOCATION: "northamerica-northeast1"
  _K6_DURATION: "8m"
  _K6_VUS: "2000"
  _BQ_DATASET_NAME: "perftest_dataset"
  _BQ_DATASET_FORMAT: "NEWLINE_DELIMITED_JSON"
  _BQ_DATASET_DESC: "perftest"
  _BQ_TABLE_NAME: "perftest_table"
  _GCS_BUCKET_NAME: "perfmon_test"
  _GCS_FILE_NAME: "metrics.json"
